{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Dunkers Banner](https://github.com/PS43Foundation/data-dunkers/blob/main/docs/top-banner.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fdata-dunkers%2Fdata-dunkers-modules&branch=main&subPath=AI/visualization.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a><a href=\"https://colab.research.google.com/github/data-dunkers/data-dunkers-modules/blob/mainAI/visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Visualization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Students will be able to:\n",
    "\n",
    "- use the [plotly](https://plotly.com/python/) library to find different visualization techniques\n",
    "- identify potential sources of error in datasets, and fix them before visualizing data\n",
    "- learn about \"K-means clustering\" and how to utilize the specific modelling technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will be presented different ways of visualizing data, specifically being shown one technique known as **\"K-means clustering\"**. \n",
    "\n",
    "As an simplified explanation, k-means clustering is a way to group data points into clusters, where each group contains data points that are similar to each other. It works by finding the center of each cluster and then assigning every data point to the nearest center.\n",
    "\n",
    "As we go through this notebook, you'll also learn different visualization methods and how we can learn more about our data using these methods. In this particular notebook, we'll be using  data from the 2023 [WNBA](https://en.wikipedia.org/wiki/Women%27s_National_Basketball_Association) regular season.\n",
    "\n",
    "Note: (you can incorporate your own datasets instead of using the own in this particular notebook)\n",
    "\n",
    "Let's start by importing our libraries necessary for the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset we'll be using in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnba_players = pd.read_csv('https://raw.githubusercontent.com/Data-Dunkers/data-dunkers-modules/main/data/wnba_player_stats_2023.csv')\n",
    "wnba_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have the name of WNBA players with their stats for the 2023 WNBA regular season.\n",
    "\n",
    "Let's take a look at all the different columns in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnba_players.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of an WNBA game, each column in our dataset represents specific statistics about a player's performance. For example, `PTS` is the player's average points per game, `Name` is the player's name, `GP` is their total amount of games played, and so on and so forth.\n",
    "\n",
    "We'll be using these numerical columns in different visualizations throughout this notebook and trying to potentially find correlations between the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "Before we get started into any data visualization, we need to clean our data.\n",
    "\n",
    "**Data cleaning** is the process of fixing or removing incorrect, corrupted, or irrelevant data from a dataset. This ensures that the data is accurate and ready for analysis or machine learning tasks.\n",
    "\n",
    "In our particular case, we need to get rid of any numerical columns that are irrelevant for \"k-means clustering\" and other visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnba_players = wnba_players.drop(['Year', \"GP\", \"MIN\"], axis=1)\n",
    "wnba_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've removed these columns, let's find start with some basic analysis.\n",
    "\n",
    "Let's find the average value for each statistical measure in the WNBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = wnba_players.select_dtypes(include=['number']).columns\n",
    "\n",
    "columns_to_exclude = ['Name', 'POS']\n",
    "numeric_columns = [col for col in numeric_columns if col not in columns_to_exclude]\n",
    "\n",
    "\n",
    "mean_values = wnba_players[numeric_columns].mean()\n",
    "mean_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By finding the average value of each numerical column in our dataset, we can get a sense of the typical or central values, which helps us understand the overall trend of the data. This is important because it allows us to quickly grasp the general patterns and make informed decisions or observations based on the data's tendencies.\n",
    "\n",
    "What kind of understandings can you find based on the average value of each statistical metric in the WNBA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create an comparison scatter-plot between each player's points, assists, and rebounds in the WNBA. We'll start our visualizations using these three metrics as they are general metrics that most basketball players can understand. \n",
    "\n",
    "In this plot we can observe whether players who score more points also tend to have more assists or rebounds and vice-versa. We can potentially gain insights into the overall performance and playing style of different players, helping us identify trends and patterns within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix = px.scatter_matrix(wnba_players[['AST', 'PTS', 'REB']], dimensions=['AST', 'PTS', 'REB'],\n",
    "                                   title=\"Plot of AST, PTS, and REB\")\n",
    "\n",
    "scatter_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our comparison plots, it appears difficult to see any major correlations between points, assists, and rebounds. \n",
    "\n",
    "Let's take a look at a [heat-map](https://en.wikipedia.org/wiki/Heat_map) and get specific numerical analysis on trends between these three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = wnba_players[['AST', 'PTS', 'REB']].corr()\n",
    "heatmap = px.imshow(corr,color_continuous_scale='Viridis',text_auto=True,  title='Heatmap of AST, PTS, and REB')\n",
    "\n",
    "heatmap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our heatmap, we see that our strongest correlation is between `PTS` and `REB` (0.62 or 62%), and our weakest correlation is between `AST` and `REB` (0.26 or 26%). \n",
    "\n",
    "Why do you think there is a stronger correlation between points (`PTS`) and rebounds (`REB`) compared to the weaker correlation between assists (`AST`) and rebounds (`REB`)? Think of what roles in the WNBA would typically get higher rebounds, and then compare the role of those players to players who get higher amounts of assists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "Now that we have a general grasp of visualization techniques to compare different columns in our dataset, let's get into **K-means clustering**. As mentioned before, this technique helps us group similar data points together, making it easier to identify patterns or trends within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=5, random_state=1)\n",
    "numeric_cols = wnba_players._get_numeric_data()\n",
    "kmeans_model.fit(numeric_cols)\n",
    "labels = kmeans_model.labels_ # get the cluster labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what the code above is doing and simplify each step:\n",
    "\n",
    "```python\n",
    "kmeans_model = KMeans(n_clusters=5, random_state=1)\n",
    "```\n",
    "\n",
    "In the code above, we start by creating a K-means model that will divide our data into 5 clusters (n_clusters=5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "numeric_cols = wnba_players._get_numeric_data()\n",
    "```\n",
    "\n",
    "In this portion of code, we're simply fetching the numerical columns in our data. These columns are: `PTS` `FGM` `FGA` `FG%` `3PM` `3PA` `3P%` `FTM` `FTA` `FT%` `REB` `AST` `STL` `BLK` `TO` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "kmeans_model.fit(numeric_cols)\n",
    "labels = kmeans_model.labels_ # get the cluster labels\n",
    "```\n",
    "\n",
    "Once the model is trained on our data, it assigns a cluster label to each data point, indicating which of the 5 groups it belongs to. These labels can then be used to analyze how players with similar characteristics are grouped together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 3, 2,\n",
    "       2, 2, 2, 2, 2, 2, 2, 0, 3, 2, 1, 0, 2, 0, 2, 3, 3, 2, 3, 0, 1, 0,\n",
    "       3, 0, 3, 3, 3, 3, 3, 0, 0, 3, 3, 1, 0, 0, 3, 3, 3, 0, 3, 3, 0, 3,\n",
    "       3, 3, 0, 3, 0, 3, 3, 0, 3, 3, 1, 3, 1, 0, 3, 1, 3, 3, 0, 1, 1, 0,\n",
    "       0, 3, 3, 0, 1, 3, 3, 0, 1, 1, 0, 4, 3, 1, 1, 0, 1, 0, 0, 1])\n",
    "```\n",
    "\n",
    "This final label output is simply showing us the different labels that our of our data-points were assigned in our `wnba_data` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we're going to utilize an technique called **principle component analysis** **(PCA)**.\n",
    "\n",
    "In simple terms, this technique simplifies data by reducing the number of variables (or dimensions). It essentially takes our original dataframe with all our numerical columns and transforms it into a new set of fewer columns that still capture the main patterns in our data. \n",
    "\n",
    "This step can be skipped as well if it is too difficult to implement in your own coding projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(2)\n",
    "plot_columns = pca_2.fit_transform(numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize our K-means visualization! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(plot_columns, columns=['PC1', 'PC2'])\n",
    "temp['Cluster'] = labels\n",
    "temp['Name'] = wnba_players['Name'] \n",
    "\n",
    "k_means = px.scatter(\n",
    "    temp,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='Cluster',\n",
    "    hover_data={'PC1': True, 'PC2': True, 'Cluster': True, 'Name': True}, \n",
    "    title='K-Means Clustering of WNBA Players',\n",
    "    color_continuous_scale=px.colors.qualitative.Plotly, \n",
    "    category_orders={'Cluster': list(range(5))}  \n",
    ")\n",
    "\n",
    "k_means.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our visualization, we see that our WNBA players are now sorted into 5 different clusters. \n",
    "\n",
    "Generally, since K-means clustering is an algorithm used on unlabeled data, we need to go further in our analysis to identify and understand any correlations or patterns within these clusters. This might involve examining the characteristics of each cluster to see what common traits or statistics are shared among players in the same group, which can provide valuable insights into player performance and grouping trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored various data visualization techniques, including scatter plots and heatmaps, to understand player statistics in the WNBA. We then applied K-means clustering to group players into distinct clusters based on their performance metrics. By analyzing these clusters and using Principal Component Analysis (PCA) to simplify the data, we gained deeper insights into player performance and identified patterns in our data.\n",
    "\n",
    "In your projects, find datasets that have useful features that can be used in the context of clustering. Many datasets can be found on [Kaggle](https://www.kaggle.com/), a platform that offers a wide range of data for analysis and experimentation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
